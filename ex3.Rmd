---
title: "Data Collection and Network Analysis"
author: "Shaked & Assaf"
date: "May 2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, root.dir='DS2017B/Gray')
library(igraph)
library(flexclust)
library(twitteR)
library(tm)
library(stringr)
```


# Centerality Measure (Question 1a)

### Load Graph

In order to be able to understand properly the relationships in "Grey's Anatomy", we'll load the data into a graph representation, which will also make it easier to explore some new information about the data easily.

```{r}
ga.data <- read.csv('ga_edgelist.csv', header=T, stringsAsFactors=F)
ga.vrtx <- read.csv('ga_actors.csv', header=T, stringsAsFactors=F)
g <- graph.data.frame(ga.data, vertices=ga.vrtx, directed=F)
g
```

### Plot Graph

Now we see that we have an **undirected** graph, with **32 vertices** and **34 edges**.  
Each vertex has information about the *name* of the person and his/her *gender*.  
Let's plot the graph, while males are in blue and females are in pink:

```{r}
V(g)$size <- 10 # Set size to all nodes
V(g)$color <- "powderblue"
females <- which(V(g)$gender == "F")
V(g)$color[females] <- "pink" 
plot(g)
```

### Largest Connectivity Component

We can see that we have **3** connectivity components. One of them is quite large, while the other two are smaller. Let's take out the largest one and plot it.

```{r}
comp <- components(g, mode="strong")
largest_comp <- which.max(comp$csize)
g_lc <- induced.subgraph(g, V(g)[which(comp$membership == largest_comp)])
plot(g_lc)
```

Now We want to calculate the centrality of the largest connectivity component, based on three measures: *betweenness*, *closeness* and *eigenvector*.

### Betweeness

Betweenness measures how many times a vertex acts as a bridge in the shortest paths between each pair of vertices.  
For each pair of vertices (S, T) that differ from X, we calculate the number of shortests paths between S and T that pass through X, divided by the number the number of shortests paths between S and T. Then, we sum it all the results and we get the betweenness of vertex X.  
There is also a normalized form of this formula, which we used, where we divide the result in the number of all possible pairs (i.e. (N-1)(N-2)/2, where N is the number of vertices).

```{r}
bw <- betweenness(g_lc, v = V(g_lc), directed=F, normalized=T)
bw <- sort(bw, decreasing=T)
bw
```

We can see that in this measure *sloan*, *karev* and *altman* have the largest betweeness centrality.

### Closeness

Closeness measures how close a vertex to all other vertices.  
It is calculated using the following formula:  
$$C(X) = \frac{1}{\sum_{Y \in V} d(X,Y)}$$
where d(x,y) is the length of the shortest path between vertex X and vertex Y.
We will use the normalized form of this formula, which look like this (N - num of vertices): 
$$C(X) = \frac{N-1}{\sum_{Y \in V} d(X,Y)}$$

```{r}
cl <- closeness(g_lc, vids = V(g_lc), normalized=T)
cl <- sort(cl, decreasing=T)
cl
```

We can see that in this measure *torres*, *addison* and *karev* have the largest closeness centrality.

### Eigenvector

Eigenvector measures the influence of a vertex in a graph.  
In this measure, how central a vertex is depends on how central its neighbors are.
It is calculated as the sum of weights of all neighbors of X, multiplied by a contant.

```{r}
ec <- eigen_centrality(g_lc, directed=F, scale=F)
ec <- sort(ec$vector, decreasing=T)
ec
```

We can see that in this measure *karev*, *torres* and *sloan* have the largest eigenvector centrality.    


# Communities (Question 1b)

## Girvan-Newman (Edge Betweenness) Clustering

Girvan-Newman is a hierarchical method that uses a TOP-DOWN approach, for finding communities in a graph, using edge betweenness.  
It is based on the idea of vertex betweenness, but this time for edges. That means, edge betweenness measures the number of shortests paths that passing on an **edge**.  
The algorithm calculates edge betweenness for the edges and then removes the edges with the highest betweenness, repeatedly, until there are no edges.

```{r}
set.seed(123)
gn <- cluster_edge_betweenness(g, directed=F)
gn
```

### Paint Graph by Communities

Now we will plot the graph, while painting each community in a unique color.

```{r}
gn_colors <- membership(gn)
plot(g, vertex.size=10, vertex.color=gn_colors, asp=F)
```

### Communities

First, we will check how many communities we got.

```{r}
gn_clusters <- length(gn)
gn_clusters
```

OK. So we know we have **7** communities. Now, we will find what is the size of each community.

```{r}
gn_communities_size <- c()
for(i in 1:gn_clusters){
  gn_communities_size <- c(gn_communities_size, length(gn[[i]]))
}
gn_communities_size
```

We can see that the communities sizes are **8, 5, 5, 4, 3, 3 and 4**.

### Modularity Score

Now, we want to see the modularity score of **Girvan-Newman** communities algorithm, which will tell us the network was partitioned into communities.  
The modularity score is:

```{r}
max(gn$modularity)
```

And the number of partitions that was made to get this modularity score is:

```{r}
which.max(gn$modularity)
```

## Louvain Clustering

Louvain Clustering is a hierarchial approach that uses a BOTTOM-UP approace for finding communities in a graph. It is a greedy algorithm.  
The algorithm works as follows:  
Initially, each vertex is assigned to a community on its own.  
In each iteration, each vertex is moved to the community which achieves the highest contribution to modularity. When no vertices can be reassigned, each community becomes a vertex and the process repeats. The alogrithms stops when there is only one vertex or when  
the modularity cannot be increased anymore.

```{r}
set.seed(123)
le <- cluster_louvain(g)
le
```

### Paint Graph by Communities

Now we will plot the graph, while painting each community in a unique color.

```{r}
le_colors <- membership(le)
plot(g, vertex.size=10, vertex.color=le_colors, asp=FALSE)
```


### Communities

First, we'll want to check how many communities we got.

```{r}
le_clusters <- length(le)
le_clusters
```

OK. So we know we have **6** communities. Now, we will find what is the size of each community.

```{r}
le_communities_size <- c()
for(i in 1:le_clusters){
  le_communities_size <- c(le_communities_size, length(le[[i]]))
}
le_communities_size
```

We can see that the communities sizes are **3, 5, 4, 5, 10 and 5**.

### Modularity Score

Now, we want to see the modularity score of **Louvain** communities algorithm, which will tell us the network was partitioned into communities.  
The modularity score is:

```{r}
max(le$modularity)
```

And the number of partitions that was made to get this modularity score is:

```{r}
which.max(le$modularity)
```


# Network Analysis (Question 2)

In this part of the assignment, we are going to perform social network analysis to tweet content that were posted in the popular social network - **Twitter**.  
We will find the frequent words that appeared in tweets that contains a certain hash-tag, and see which ones appeared together in the same tweet.  
Using this analysis we can get more info about the connections between certain words.

### Setting Twitter OAuth Credentials

First, we need to set up our Twitter OAuth credentials, so we would be able to download the tweets data using the API.

```{r}
consumer_key <- "DNn8CHvzQoKeJpIotKj91CDjP"
consumer_secret <- "vdBVVZnBMYnugQiLq4d0IiqYnkqPK0YhO0MwDuw9Ph8sUdZdX7"
access_token <- "769590451-QmB6viAW6hag3H8ezRSKIwtASKQZwBWjnjXz4vBQ"
access_secret <- "KF1XP4LN6YhEbrLA2VXGU4D2tiGWl3Or6kL27I6VPem9d"
sig <- setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
```

### Download Tweets

Now, we are going to download tweets using *twitteR* package.  
We will get the latest tweets that contains the hashtag **MSBuild**, which relates to the latest Microsoft Build conference, and that are written in english.  
Dates are entered for reproducablity (as much as possible).

```{r}
search_res <- searchTwitter("#MSBuild", n=1000, lang="en", 
                            since="2017-05-01", until="2017-05-12")
```

### Data Preprocessing

Now that we have the data, we want to filter it and use only the relevant data for our social network analysis.  
We will perform our data cleaning using the following steps:  
1. Extract the text of the tweet.  
2. Remove all retweets (to avoid duplications).  
3. Remove graphical characters from each tweet (such as emojis).  
4. Remove punctuations from each tweet.  
5. Remove stop words from each tweet.  

```{r}
search_text <- sapply(search_res, function(x) x$getText())
search_text <- search_text[!grepl("RT ", search_text)]
search_text <- sapply(search_text, function(x) str_replace_all(x, "[^[:graph:]]", " "))
search_text <- sapply(search_text, removePunctuation)
search_text <- sapply(search_text, function(x) removeWords(x, stopwords('english')))
```

Now, we will create a *TermDocumentMatrix* object, that performs term frequency analysis in each tweet. This time a tween is considered a document.

```{r}
search_text_corpus <- Corpus(VectorSource(search_text))
tdm <- TermDocumentMatrix(search_text_corpus)
```

Using our *TermDocumentMatrix*, we will extract the terms that have more than **10** occurrences in our tweets.

```{r}
term_freq = findFreqTerms(tdm, lowfreq = 10)
term_freq
```

Now, we will create a new *weighted* adjacency matrix to the frequent words we found, using our term frequency matrix.  
To do so, we will first filter only these terms, then we will make the frequency binary and then create a new term-term matrix, using matrix multiplication of our matrix and its transposed matrix.  
This is a part of the adjacency matrix:

```{r}
tdm_freq <- tdm[term_freq,]
tdm_freq[tdm_freq > 1] <- 1
tdm_freq <- as.matrix(tdm_freq)
tm <- tdm_freq %*% t(tdm_freq)
tm[1:9,1:9]
```

### Graph Creation

With the adjacenct matrix we created before, we will now create a weighted undirected graph, to display our data. In this case, the terms will be the vertices of our graph and the edges will be between each two vertices that theirs terms where appearing together in the same tweet. Hence, The weights will be the number of tweets where the two  terms appeared together.   In order to simplify the graph we will remove self edges from the graph, and in order to demonstrate the significance of each term, we will make the size of the node proportional to its degree.

```{r}
g1 = graph.adjacency(tm, weighted=T, mode="undirected")
g1 <- simplify(g1)
V(g1)$size <- degree(g1)/1.5
V(g1)$color <- "powderblue"
plot(g1, layout=layout.fruchterman.reingold, asp=F)
```

### Calculate Centrality

Now, we want to calculate betweenness, closeness and eigenvector centralities to our new graph, in order to see how important and how significant each vertex is to the graph.  
In every result, the centrality values are sorted from the highest to the lowest.

#### Betweenness

```{r}
bw <- betweenness(g1, v = V(g1), directed=F, normalized=T, weights=E(g1)$weight)
bw <- sort(bw, decreasing=T)
bw
```

#### Closeness

```{r}
cl <- closeness(g1, vids = V(g1), normalized=T, weights=E(g1)$weight)
cl <- sort(cl, decreasing=T)
cl
```

#### Eigenvector

```{r}
ec <- eigen_centrality(g1, directed=F, scale=F, weights=E(g1)$weight)
ec <- sort(ec$vector, decreasing=T)
ec
```

### Clustering

### Girvan-Newman (Edge Betweenness) Clustering

Girvan-Newman is a hierarchical method that uses a TOP-DOWN approach, for finding communities in a graph, using edge betweenness.  
It is based on the idea of vertex betweenness, but this time for edges. That means, edge betweenness measures the number of shortests paths that passing on an **edge**.  
The algorithm calculates edge betweenness for the edges and then removes the edges with the highest betweenness, repeatedly, until there are no edges.

```{r}
set.seed(123)
gn <- cluster_edge_betweenness(g1, directed=F, weights=E(g1)$weight)
gn
```

#### Paint Graph by Communities

Now we will plot the graph, while painting each community in a unique color.

```{r}
gn_colors <- membership(gn)
plot(g1, vertex.size=10, vertex.color=gn_colors, asp=F)
```

#### Communities

First, we will check how many communities we got.

```{r}
gn_clusters <- length(gn)
gn_clusters
```

OK. So we know we have **7** communities. Now, we will find what is the size of each community.

```{r}
gn_communities_size <- c()
for(i in 1:gn_clusters){
  gn_communities_size <- c(gn_communities_size, length(gn[[i]]))
}
gn_communities_size
```

We can see that the communities sizes are **6, 4, 9, 4, 2, 1 and 1**.

#### Modularity Score

Now, we want to see the modularity score of **Girvan-Newman** communities algorithm, which will tell us the network was partitioned into communities.  
The modularity score is:

```{r}
max(gn$modularity)
```

And the number of partitions that was made to get this modularity score is:

```{r}
which.max(gn$modularity)
```

### Louvain Clustering

Louvain Clustering is a hierarchial approach that uses a BOTTOM-UP approace for finding communities in a graph. It is a greedy algorithm.  
The algorithm works as follows:  
Initially, each vertex is assigned to a community on its own.  
In each iteration, each vertex is moved to the community which achieves the highest contribution to modularity. When no vertices can be reassigned, each community becomes a vertex and the process repeats. The alogrithms stops when there is only one vertex or when the modularity cannot be increased anymore.

```{r}
set.seed(123)
le <- cluster_louvain(g1)
le
```

#### Paint Graph by Communities

Now we will plot the graph, while painting each community in a unique color.

```{r}
le_colors <- membership(le)
plot(g1, vertex.size=10, vertex.color=le_colors, asp=FALSE)
```


#### Communities

First, we'll want to check how many communities we got.

```{r}
le_clusters <- length(le)
le_clusters
```

OK. So we know we have **3** communities. Now, we will find what is the size of each community.

```{r}
le_communities_size <- c()
for(i in 1:le_clusters){
  le_communities_size <- c(le_communities_size, length(le[[i]]))
}
le_communities_size
```

We can see that the communities sizes are **4, 18 and 5**.

#### Modularity Score

Now, we want to see the modularity score of **Louvain** communities algorithm, which will tell us the network was partitioned into communities.  
The modularity score is:

```{r}
max(le$modularity)
```

And the number of partitions that was made to get this modularity score is:

```{r}
which.max(le$modularity)
```
